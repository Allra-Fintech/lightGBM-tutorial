{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22c6ae9",
   "metadata": {},
   "source": [
    "# LightGBM Ads Tutorial\n",
    "\n",
    "End-to-end tutorial using synthetic keyword-ads performance data.\n",
    "\n",
    "**Models covered:**\n",
    "1. CTR prediction (regression)\n",
    "2. Conversion prediction (binary classification)\n",
    "3. Keyword ranking function (score-based)\n",
    "4. Learning-to-Rank with LambdaMART (group-split version)\n",
    "5. Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df58ec9",
   "metadata": {},
   "source": [
    "## 0) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884d28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27814e01",
   "metadata": {},
   "source": [
    "## 1) Create synthetic ads dataset\n",
    "\n",
    "Each row represents one `(given_word, keyword)` pair with features:\n",
    "- `similarity` â€“ cosine-like similarity between the two words\n",
    "- `competition`, `impressions`, `clicks`, `cpc`, `cost`, `device`, `hour`\n",
    "\n",
    "Targets:\n",
    "- `ctr` â€“ click-through rate (regression)\n",
    "- `has_conversion` â€“ did it convert at least once? (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63d9876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuzong/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given_words : 50\n",
      "keywords    : 1030\n",
      "Encoding embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec78b4f90f9484d8787c9d67f8cf72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "\n",
      "Dataset shape: (100000, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_word</th>\n",
       "      <th>keyword</th>\n",
       "      <th>similarity</th>\n",
       "      <th>competition</th>\n",
       "      <th>impressions</th>\n",
       "      <th>clicks</th>\n",
       "      <th>cpc</th>\n",
       "      <th>cost</th>\n",
       "      <th>device</th>\n",
       "      <th>hour</th>\n",
       "      <th>ctr</th>\n",
       "      <th>has_conversion</th>\n",
       "      <th>conversions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch</td>\n",
       "      <td>vacuum for pet hair</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531397</td>\n",
       "      <td>19901</td>\n",
       "      <td>172</td>\n",
       "      <td>1.804093</td>\n",
       "      <td>310.304063</td>\n",
       "      <td>desktop</td>\n",
       "      <td>13</td>\n",
       "      <td>0.008643</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>supplement</td>\n",
       "      <td>slide guitar</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470275</td>\n",
       "      <td>18780</td>\n",
       "      <td>169</td>\n",
       "      <td>1.616793</td>\n",
       "      <td>273.238024</td>\n",
       "      <td>desktop</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008999</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mattress</td>\n",
       "      <td>horseshoe necklace</td>\n",
       "      <td>0.036682</td>\n",
       "      <td>0.789980</td>\n",
       "      <td>19265</td>\n",
       "      <td>734</td>\n",
       "      <td>2.332960</td>\n",
       "      <td>1712.392556</td>\n",
       "      <td>mobile</td>\n",
       "      <td>19</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sunglasses</td>\n",
       "      <td>monitor arm</td>\n",
       "      <td>0.214580</td>\n",
       "      <td>0.766600</td>\n",
       "      <td>18955</td>\n",
       "      <td>1246</td>\n",
       "      <td>2.328759</td>\n",
       "      <td>2901.633413</td>\n",
       "      <td>mobile</td>\n",
       "      <td>23</td>\n",
       "      <td>0.065735</td>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sunglasses</td>\n",
       "      <td>pixel phone</td>\n",
       "      <td>0.228574</td>\n",
       "      <td>0.773931</td>\n",
       "      <td>17678</td>\n",
       "      <td>584</td>\n",
       "      <td>2.094634</td>\n",
       "      <td>1223.266280</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8</td>\n",
       "      <td>0.033035</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   given_word              keyword  similarity  competition  impressions  \\\n",
       "0       watch  vacuum for pet hair    0.000000     0.531397        19901   \n",
       "1  supplement         slide guitar    0.000000     0.470275        18780   \n",
       "2    mattress   horseshoe necklace    0.036682     0.789980        19265   \n",
       "3  sunglasses          monitor arm    0.214580     0.766600        18955   \n",
       "4  sunglasses          pixel phone    0.228574     0.773931        17678   \n",
       "\n",
       "   clicks       cpc         cost   device  hour       ctr  has_conversion  \\\n",
       "0     172  1.804093   310.304063  desktop    13  0.008643               1   \n",
       "1     169  1.616793   273.238024  desktop     1  0.008999               1   \n",
       "2     734  2.332960  1712.392556   mobile    19  0.038100               1   \n",
       "3    1246  2.328759  2901.633413   mobile    23  0.065735               1   \n",
       "4     584  2.094634  1223.266280  desktop     8  0.033035               1   \n",
       "\n",
       "   conversions  \n",
       "0            9  \n",
       "1           14  \n",
       "2           45  \n",
       "3          149  \n",
       "4           70  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Load word lists from files\n",
    "with open(\"given_words.json\") as f:\n",
    "    given_words = np.array(json.load(f))\n",
    "\n",
    "with open(\"keywords.json\") as f:\n",
    "    keywords = np.array(json.load(f))\n",
    "\n",
    "print(f\"given_words : {len(given_words)}\")\n",
    "print(f\"keywords    : {len(keywords)}\")\n",
    "\n",
    "# â”€â”€ Precompute real similarities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"Encoding embeddings...\")\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "all_words = np.unique(np.concatenate([given_words, keywords]))\n",
    "vecs      = embed_model.encode(all_words.tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "vec_index = dict(zip(all_words, vecs))\n",
    "\n",
    "def real_similarity(given: np.ndarray, kw: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Vectorised cosine similarity for arrays of given/keyword strings.\"\"\"\n",
    "    g_vecs = np.stack([vec_index[g] for g in given])\n",
    "    k_vecs = np.stack([vec_index[k] for k in kw])\n",
    "    # row-wise dot product (vectors are already normalised)\n",
    "    return np.clip((g_vecs * k_vecs).sum(axis=1), 0.0, 1.0)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "def make_ads_dataset(n=100_000):\n",
    "    given = rng.choice(given_words, size=n)\n",
    "    kw    = rng.choice(keywords, size=n)\n",
    "\n",
    "    similarity  = real_similarity(given, kw)\n",
    "\n",
    "    impressions = rng.integers(50, 20000, size=n)\n",
    "    device      = rng.choice([\"mobile\", \"desktop\"], size=n, p=[0.7, 0.3])\n",
    "    hour        = rng.integers(0, 24, size=n)\n",
    "    competition = rng.uniform(0.1, 1.0, size=n)\n",
    "\n",
    "    cpc = np.clip(\n",
    "        0.2 + 2.0 * competition + 0.5 * (1 - similarity) + rng.normal(0, 0.15, size=n),\n",
    "        0.05, None\n",
    "    )\n",
    "\n",
    "    device_boost = np.where(device == \"mobile\", 0.02, 0.0)\n",
    "    hour_boost   = np.where((hour >= 19) & (hour <= 23), 0.01, 0.0)\n",
    "    ctr = np.clip(\n",
    "        0.01 + 0.10 * similarity + device_boost + hour_boost + rng.normal(0, 0.01, size=n),\n",
    "        0.0005, 0.30\n",
    "    )\n",
    "\n",
    "    clicks = rng.binomial(impressions, p=ctr)\n",
    "    cost   = clicks * cpc\n",
    "\n",
    "    conv_p = 1 / (1 + np.exp(-(-2.0 + 4.0 * similarity - 0.4 * cpc)))\n",
    "    conversions    = rng.binomial(np.maximum(clicks, 1), p=np.clip(conv_p, 0.0001, 0.8))\n",
    "    has_conversion = (conversions > 0).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"given_word\":     given,\n",
    "        \"keyword\":        kw,\n",
    "        \"similarity\":     similarity,\n",
    "        \"competition\":    competition,\n",
    "        \"impressions\":    impressions,\n",
    "        \"clicks\":         clicks,\n",
    "        \"cpc\":            cpc,\n",
    "        \"cost\":           cost,\n",
    "        \"device\":         device,\n",
    "        \"hour\":           hour,\n",
    "        \"ctr\":            np.where(impressions > 0, clicks / impressions, 0.0),\n",
    "        \"has_conversion\": has_conversion,\n",
    "        \"conversions\":    conversions,\n",
    "    })\n",
    "\n",
    "df = make_ads_dataset(100_000)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1988b4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>competition</th>\n",
       "      <th>impressions</th>\n",
       "      <th>clicks</th>\n",
       "      <th>cpc</th>\n",
       "      <th>cost</th>\n",
       "      <th>hour</th>\n",
       "      <th>ctr</th>\n",
       "      <th>has_conversion</th>\n",
       "      <th>conversions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.197328</td>\n",
       "      <td>0.548878</td>\n",
       "      <td>10008.157780</td>\n",
       "      <td>458.055210</td>\n",
       "      <td>1.698776</td>\n",
       "      <td>771.320205</td>\n",
       "      <td>11.472930</td>\n",
       "      <td>0.045756</td>\n",
       "      <td>0.985570</td>\n",
       "      <td>73.912030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.115925</td>\n",
       "      <td>0.259774</td>\n",
       "      <td>5753.572191</td>\n",
       "      <td>338.596249</td>\n",
       "      <td>0.544315</td>\n",
       "      <td>635.426607</td>\n",
       "      <td>6.897224</td>\n",
       "      <td>0.018704</td>\n",
       "      <td>0.119256</td>\n",
       "      <td>102.030089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.278226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.122960</td>\n",
       "      <td>0.323847</td>\n",
       "      <td>5015.750000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>1.248251</td>\n",
       "      <td>280.721762</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.033165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.184707</td>\n",
       "      <td>0.548076</td>\n",
       "      <td>9987.500000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>1.699685</td>\n",
       "      <td>615.572365</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.045356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.250362</td>\n",
       "      <td>0.773605</td>\n",
       "      <td>14981.000000</td>\n",
       "      <td>670.000000</td>\n",
       "      <td>2.149420</td>\n",
       "      <td>1100.535014</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.057424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.903308</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>19999.000000</td>\n",
       "      <td>2560.000000</td>\n",
       "      <td>3.152396</td>\n",
       "      <td>5077.750616</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1931.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          similarity    competition    impressions         clicks  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.197328       0.548878   10008.157780     458.055210   \n",
       "std         0.115925       0.259774    5753.572191     338.596249   \n",
       "min         0.000000       0.100003      50.000000       0.000000   \n",
       "25%         0.122960       0.323847    5015.750000     183.000000   \n",
       "50%         0.184707       0.548076    9987.500000     397.000000   \n",
       "75%         0.250362       0.773605   14981.000000     670.000000   \n",
       "max         0.903308       0.999987   19999.000000    2560.000000   \n",
       "\n",
       "                 cpc           cost           hour            ctr  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        1.698776     771.320205      11.472930       0.045756   \n",
       "std         0.544315     635.426607       6.897224       0.018704   \n",
       "min         0.278226       0.000000       0.000000       0.000000   \n",
       "25%         1.248251     280.721762       6.000000       0.033165   \n",
       "50%         1.699685     615.572365      11.000000       0.045356   \n",
       "75%         2.149420    1100.535014      17.000000       0.057424   \n",
       "max         3.152396    5077.750616      23.000000       0.166667   \n",
       "\n",
       "       has_conversion    conversions  \n",
       "count   100000.000000  100000.000000  \n",
       "mean         0.985570      73.912030  \n",
       "std          0.119256     102.030089  \n",
       "min          0.000000       0.000000  \n",
       "25%          1.000000      20.000000  \n",
       "50%          1.000000      47.000000  \n",
       "75%          1.000000      92.000000  \n",
       "max          1.000000    1931.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21903c20",
   "metadata": {},
   "source": [
    "## 2) Prepare features\n",
    "\n",
    "LightGBM handles categorical features natively when they are `pandas.Categorical` dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16f0637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (80000, 10)  |  Test: (20000, 10)\n",
      "Conversion rate (train): 0.985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"given_word\", \"keyword\", \"similarity\", \"competition\",\n",
    "    \"impressions\", \"clicks\", \"cpc\", \"cost\", \"device\", \"hour\"\n",
    "]\n",
    "CAT_COLS = [\"given_word\", \"keyword\", \"device\"]\n",
    "\n",
    "X = df[FEATURE_COLS].copy()\n",
    "for c in CAT_COLS:\n",
    "    X[c] = X[c].astype(\"category\")\n",
    "\n",
    "y_ctr  = df[\"ctr\"].values\n",
    "y_conv = df[\"has_conversion\"].values\n",
    "\n",
    "X_train, X_test, y_ctr_train, y_ctr_test = train_test_split(\n",
    "    X, y_ctr, test_size=0.2, random_state=42\n",
    ")\n",
    "# reuse the same split indices for the conversion target\n",
    "y_conv_train = y_conv[X_train.index]\n",
    "y_conv_test  = y_conv[X_test.index]\n",
    "\n",
    "print(f\"Train: {X_train.shape}  |  Test: {X_test.shape}\")\n",
    "print(f\"Conversion rate (train): {y_conv_train.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde0599",
   "metadata": {},
   "source": [
    "## 3) Model A â€” CTR prediction (regression)\n",
    "\n",
    "CTR is continuous and bounded in (0, 1). We weight each sample by `impressions` so high-volume rows have more influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2ef3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l2: 2.67262e-06\n",
      "[400]\tvalid_0's l2: 1.22088e-06\n",
      "[600]\tvalid_0's l2: 9.41585e-07\n",
      "[800]\tvalid_0's l2: 8.53782e-07\n",
      "[1000]\tvalid_0's l2: 8.20225e-07\n",
      "[1200]\tvalid_0's l2: 8.00581e-07\n",
      "[1400]\tvalid_0's l2: 7.89481e-07\n",
      "[1600]\tvalid_0's l2: 7.82909e-07\n",
      "[1800]\tvalid_0's l2: 7.77716e-07\n",
      "[2000]\tvalid_0's l2: 7.739e-07\n",
      "\n",
      "CTR RMSE : 0.001495\n",
      "Best iter: 1992\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = lgb.LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "reg.fit(\n",
    "    X_train, y_ctr_train,\n",
    "    sample_weight=X_train[\"impressions\"],\n",
    "    eval_set=[(X_test, y_ctr_test)],\n",
    "    eval_sample_weight=[X_test[\"impressions\"]],\n",
    "    eval_metric=\"l2\",\n",
    "    categorical_feature=CAT_COLS,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=80, verbose=False),\n",
    "               lgb.log_evaluation(period=200)],\n",
    ")\n",
    "\n",
    "pred_ctr = reg.predict(X_test)\n",
    "rmse = mean_squared_error(y_ctr_test, pred_ctr) ** 0.5\n",
    "print(f\"\\nCTR RMSE : {rmse:.6f}\")\n",
    "print(f\"Best iter: {reg.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cfe2d",
   "metadata": {},
   "source": [
    "## 4) Model B â€” Conversion prediction (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbe96b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC   : 0.9935\n",
      "PR-AUC: 0.9999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train, y_conv_train,\n",
    "    eval_set=[(X_test, y_conv_test)],\n",
    "    eval_metric=\"auc\",\n",
    "    categorical_feature=CAT_COLS,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "               lgb.log_evaluation(period=200)],\n",
    ")\n",
    "\n",
    "proba = clf.predict_proba(X_test)[:, 1]\n",
    "print(f\"\\nAUC   : {roc_auc_score(y_conv_test, proba):.4f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_conv_test, proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4502d",
   "metadata": {},
   "source": [
    "## 5) Rank keywords for a given word\n",
    "\n",
    "For a new `given_word`, score a list of candidate keywords using:\n",
    "- `pred_ctr` from the regression model\n",
    "- `pred_conv_prob` from the classifier\n",
    "- `score = pred_ctr Ã— pred_conv_prob` (customize to ROAS, profit, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926eba49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_word</th>\n",
       "      <th>keyword</th>\n",
       "      <th>similarity</th>\n",
       "      <th>pred_ctr</th>\n",
       "      <th>pred_conv_prob</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sneakers</td>\n",
       "      <td>white sneakers</td>\n",
       "      <td>0.858920</td>\n",
       "      <td>0.033682</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.011272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sneakers</td>\n",
       "      <td>running shoes</td>\n",
       "      <td>0.669356</td>\n",
       "      <td>0.029309</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.009809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sneakers</td>\n",
       "      <td>canvas shoes</td>\n",
       "      <td>0.649999</td>\n",
       "      <td>0.028859</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.009658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sneakers</td>\n",
       "      <td>hiking boots</td>\n",
       "      <td>0.582462</td>\n",
       "      <td>0.026393</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.008833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sneakers</td>\n",
       "      <td>leather wallet</td>\n",
       "      <td>0.331565</td>\n",
       "      <td>0.015570</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.005211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sneakers</td>\n",
       "      <td>gaming mouse</td>\n",
       "      <td>0.274997</td>\n",
       "      <td>0.011825</td>\n",
       "      <td>0.261867</td>\n",
       "      <td>0.003097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sneakers</td>\n",
       "      <td>wireless earbuds</td>\n",
       "      <td>0.180678</td>\n",
       "      <td>0.006974</td>\n",
       "      <td>0.268278</td>\n",
       "      <td>0.001871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sneakers</td>\n",
       "      <td>yoga mat</td>\n",
       "      <td>0.166128</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>0.247677</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  given_word           keyword  similarity  pred_ctr  pred_conv_prob     score\n",
       "0   sneakers    white sneakers    0.858920  0.033682        0.334675  0.011272\n",
       "1   sneakers     running shoes    0.669356  0.029309        0.334675  0.009809\n",
       "2   sneakers      canvas shoes    0.649999  0.028859        0.334675  0.009658\n",
       "3   sneakers      hiking boots    0.582462  0.026393        0.334675  0.008833\n",
       "4   sneakers    leather wallet    0.331565  0.015570        0.334675  0.005211\n",
       "5   sneakers      gaming mouse    0.274997  0.011825        0.261867  0.003097\n",
       "6   sneakers  wireless earbuds    0.180678  0.006974        0.268278  0.001871\n",
       "7   sneakers          yoga mat    0.166128  0.006350        0.247677  0.001573"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rank_keywords_for_given(given_word: str, candidates: list, base_features: dict) -> pd.DataFrame:\n",
    "    \"\"\"Score and rank candidate keywords for a given word.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    given_word    : The query / seed word.\n",
    "    candidates    : List of keyword strings to evaluate.\n",
    "    base_features : Dict of feature values shared across all candidates\n",
    "                    (all cols except given_word, keyword, similarity).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame sorted by score descending.\n",
    "    \"\"\"\n",
    "    # Compute real similarities using sentence-transformers\n",
    "    texts     = [given_word] + candidates\n",
    "    vecs      = embed_model.encode(texts, normalize_embeddings=True)\n",
    "    given_vec = vecs[0:1]\n",
    "    kw_vecs   = vecs[1:]\n",
    "    sims      = cosine_similarity(given_vec, kw_vecs)[0]\n",
    "\n",
    "    rows = [\n",
    "        {**base_features, \"given_word\": given_word, \"keyword\": kw, \"similarity\": float(sim)}\n",
    "        for kw, sim in zip(candidates, sims)\n",
    "    ]\n",
    "    Xcand = pd.DataFrame(rows)[FEATURE_COLS]\n",
    "    for c in CAT_COLS:\n",
    "        Xcand[c] = Xcand[c].astype(\"category\")\n",
    "\n",
    "    ctr_hat  = reg.predict(Xcand)\n",
    "    conv_hat = clf.predict_proba(Xcand)[:, 1]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"given_word\":     given_word,\n",
    "        \"keyword\":        candidates,\n",
    "        \"similarity\":     sims,\n",
    "        \"pred_ctr\":       ctr_hat,\n",
    "        \"pred_conv_prob\": conv_hat,\n",
    "        \"score\":          ctr_hat * conv_hat,\n",
    "    }).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "candidates = [\n",
    "    \"white sneakers\", \"running shoes\", \"canvas shoes\",\n",
    "    \"hiking boots\", \"yoga mat\", \"leather wallet\",\n",
    "    \"wireless earbuds\", \"gaming mouse\",\n",
    "]\n",
    "\n",
    "base = {\n",
    "    \"competition\": 0.6,\n",
    "    \"impressions\": 5000,\n",
    "    \"clicks\":      0,\n",
    "    \"cpc\":         2.0,\n",
    "    \"cost\":        0.0,\n",
    "    \"device\":      \"mobile\",\n",
    "    \"hour\":        21,\n",
    "}\n",
    "\n",
    "ranked = rank_keywords_for_given(\"sneakers\", candidates, base)\n",
    "ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32e47d",
   "metadata": {},
   "source": [
    "## 6) Learning-to-Rank with LambdaMART\n",
    "\n",
    "A proper LambdaMART setup requires:\n",
    "1. **Group-based train/test split** â€” keep all rows for a `given_word` in the same split.\n",
    "2. **Group sizes array** â€” number of candidate keywords per query, in order.\n",
    "3. **Relevance labels** â€” here we use `ctr`; in production use ROAS or conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df8cb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranker train rows: 80092  |  test rows: 19908\n",
      "Train given_words: ['boots', 'camera', 'camping', 'candle', 'coffee', 'desk', 'dress', 'fishing', 'gaming', 'gift', 'guitar', 'handbag', 'headphones', 'jacket', 'jewelry', 'keyboard', 'laptop', 'luggage', 'makeup', 'mattress', 'monitor', 'necklace', 'pants', 'perfume', 'pet', 'phone', 'plant', 'printer', 'protein', 'running', 'shoes', 'skincare', 'sneakers', 'sunglasses', 'supplement', 'toy', 'vitamin', 'wallet', 'watch', 'yoga']\n",
      "Test  given_words: ['baby', 'backpack', 'bicycle', 'blender', 'book', 'ring', 'shirt', 'sofa', 'tent', 'vacuum']\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "\n",
    "# â”€â”€ 6a) Group-based train/test split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "unique_given = df[\"given_word\"].unique()\n",
    "rng_split    = np.random.default_rng(0)\n",
    "rng_split.shuffle(unique_given)\n",
    "\n",
    "split_idx    = int(len(unique_given) * 0.8)\n",
    "train_words  = set(unique_given[:split_idx])\n",
    "test_words   = set(unique_given[split_idx:])\n",
    "\n",
    "df_rank = df.sort_values(\"given_word\").copy()\n",
    "\n",
    "mask_train = df_rank[\"given_word\"].isin(train_words)\n",
    "df_r_train = df_rank[mask_train].copy()\n",
    "df_r_test  = df_rank[~mask_train].copy()\n",
    "\n",
    "print(f\"Ranker train rows: {len(df_r_train)}  |  test rows: {len(df_r_test)}\")\n",
    "print(f\"Train given_words: {sorted(train_words)}\")\n",
    "print(f\"Test  given_words: {sorted(test_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "766f3211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label range: 0 â€“ 4  (grades 0â€“4)\n",
      "Group sizes (train): [2077, 1977, 2026, 1925, 2017, 1916, 1993, 2019, 2022, 1946, 1978, 1976, 2089, 1906, 2006, 1985, 2069, 2056, 1932, 2027, 2093, 2046, 1986, 2097, 2021, 1973, 2031, 2016, 2092, 1976, 1963, 2021, 1938, 1998, 1975, 1984, 1962, 1979, 1967, 2032]\n",
      "Group sizes (test) : [2002, 1956, 2011, 2028, 1987, 1920, 2081, 1967, 1957, 1999]\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ 6b) Build feature matrices and group size arrays â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def bin_ctr(ctr_values: np.ndarray, n_bins: int = 5) -> np.ndarray:\n",
    "    \"\"\"Convert continuous CTR into integer relevance grades (0 to n_bins-1).\"\"\"\n",
    "    bins = np.quantile(ctr_values, np.linspace(0, 1, n_bins + 1))\n",
    "    bins = np.unique(bins)  # remove duplicates if any\n",
    "    return np.digitize(ctr_values, bins[1:-1]).astype(int)\n",
    "\n",
    "def build_rank_arrays(subset: pd.DataFrame):\n",
    "    Xr = subset[FEATURE_COLS].copy()\n",
    "    for c in CAT_COLS:\n",
    "        Xr[c] = Xr[c].astype(\"category\")\n",
    "    y      = bin_ctr(subset[\"ctr\"].values)   # integer grades required by LambdaMART\n",
    "    groups = subset.groupby(\"given_word\", sort=True).size().tolist()\n",
    "    return Xr, y, groups\n",
    "\n",
    "Xr_train, yr_train, groups_train = build_rank_arrays(df_r_train)\n",
    "Xr_test,  yr_test,  groups_test  = build_rank_arrays(df_r_test)\n",
    "\n",
    "print(f\"Label range: {yr_train.min()} â€“ {yr_train.max()}  (grades 0â€“4)\")\n",
    "print(f\"Group sizes (train): {groups_train}\")\n",
    "print(f\"Group sizes (test) : {groups_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccaf5cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best iteration: 18\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ 6c) Train LambdaMART ranker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "ranker.fit(\n",
    "    Xr_train, yr_train,\n",
    "    group=groups_train,\n",
    "    eval_set=[(Xr_test, yr_test)],\n",
    "    eval_group=[groups_test],\n",
    "    eval_at=[3, 5, 10],\n",
    "    categorical_feature=CAT_COLS,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=80, verbose=False),\n",
    "               lgb.log_evaluation(period=200)],\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {ranker.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf40b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_word</th>\n",
       "      <th>keyword</th>\n",
       "      <th>ctr</th>\n",
       "      <th>ranker_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blender</td>\n",
       "      <td>ninja blender</td>\n",
       "      <td>0.098982</td>\n",
       "      <td>0.808492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blender</td>\n",
       "      <td>travel blender</td>\n",
       "      <td>0.104976</td>\n",
       "      <td>0.808492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blender</td>\n",
       "      <td>beauty blender</td>\n",
       "      <td>0.103904</td>\n",
       "      <td>0.808492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blender</td>\n",
       "      <td>countertop blender</td>\n",
       "      <td>0.079826</td>\n",
       "      <td>0.808492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blender</td>\n",
       "      <td>beauty blender</td>\n",
       "      <td>0.096589</td>\n",
       "      <td>0.808492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>blender</td>\n",
       "      <td>open back headphones</td>\n",
       "      <td>0.038789</td>\n",
       "      <td>-0.804714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>blender</td>\n",
       "      <td>leather jacket</td>\n",
       "      <td>0.047831</td>\n",
       "      <td>-0.804868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>blender</td>\n",
       "      <td>flannel shirt</td>\n",
       "      <td>0.045409</td>\n",
       "      <td>-0.805315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>blender</td>\n",
       "      <td>firm mattress</td>\n",
       "      <td>0.051174</td>\n",
       "      <td>-0.806465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>blender</td>\n",
       "      <td>opal ring</td>\n",
       "      <td>0.049958</td>\n",
       "      <td>-0.806730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2028 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     given_word               keyword       ctr  ranker_score\n",
       "0       blender         ninja blender  0.098982      0.808492\n",
       "1       blender        travel blender  0.104976      0.808492\n",
       "2       blender        beauty blender  0.103904      0.808492\n",
       "3       blender    countertop blender  0.079826      0.808492\n",
       "4       blender        beauty blender  0.096589      0.808492\n",
       "...         ...                   ...       ...           ...\n",
       "2023    blender  open back headphones  0.038789     -0.804714\n",
       "2024    blender        leather jacket  0.047831     -0.804868\n",
       "2025    blender         flannel shirt  0.045409     -0.805315\n",
       "2026    blender         firm mattress  0.051174     -0.806465\n",
       "2027    blender             opal ring  0.049958     -0.806730\n",
       "\n",
       "[2028 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# â”€â”€ 6d) Inspect ranker scores for one test given_word â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sample_word = list(test_words)[0]\n",
    "df_sample   = df_r_test[df_r_test[\"given_word\"] == sample_word].copy()\n",
    "\n",
    "Xs = df_sample[FEATURE_COLS].copy()\n",
    "for c in CAT_COLS:\n",
    "    Xs[c] = Xs[c].astype(\"category\")\n",
    "\n",
    "df_sample[\"ranker_score\"] = ranker.predict(Xs)\n",
    "df_sample[[\"given_word\", \"keyword\", \"ctr\", \"ranker_score\"]] \\\n",
    "    .sort_values(\"ranker_score\", ascending=False) \\\n",
    "    .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7c66d",
   "metadata": {},
   "source": [
    "## 7) Feature importance\n",
    "\n",
    "Using **gain** (total reduction in loss attributed to each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "025469cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CTR Regression ===\n",
      "    feature   importance\n",
      "     clicks 2.376324e+06\n",
      "impressions 8.974916e+05\n",
      " similarity 5.670585e+05\n",
      "     device 5.235285e+05\n",
      "       cost 1.272965e+05\n",
      "        cpc 4.867025e+04\n",
      "       hour 3.862799e+04\n",
      "    keyword 1.774280e+04\n",
      " given_word 6.949172e+03\n",
      "competition 2.458634e+03\n",
      "\n",
      "=== Conversion Classifier ===\n",
      "    feature    importance\n",
      "     clicks 114545.429480\n",
      "       cost  17831.133654\n",
      " similarity   7060.908456\n",
      "        cpc   5954.600694\n",
      "competition   4271.569825\n",
      "impressions   3338.817583\n",
      "       hour   2226.812948\n",
      "    keyword   1600.451376\n",
      " given_word    522.214152\n",
      "     device    284.730831\n",
      "\n",
      "=== LambdaMART Ranker ===\n",
      "    feature  importance\n",
      "     clicks 7653.807459\n",
      "       cost 1088.720889\n",
      " similarity  460.325612\n",
      "impressions  195.929314\n",
      "        cpc  168.420746\n",
      "     device  159.497969\n",
      "competition   28.083892\n",
      "    keyword   24.707920\n",
      "       hour   23.041285\n",
      " given_word    0.000000\n"
     ]
    }
   ],
   "source": [
    "def show_importance(model, title: str):\n",
    "    fi = pd.DataFrame({\n",
    "        \"feature\":    model.feature_name_,\n",
    "        \"importance\": model.booster_.feature_importance(importance_type=\"gain\"),\n",
    "    }).sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(fi.to_string(index=False))\n",
    "    return fi\n",
    "\n",
    "fi_reg    = show_importance(reg,    \"CTR Regression\")\n",
    "fi_clf    = show_importance(clf,    \"Conversion Classifier\")\n",
    "fi_ranker = show_importance(ranker, \"LambdaMART Ranker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6408d6-e9ad-457a-8cda-a967666b2642",
   "metadata": {},
   "source": "## Quick-reference: choosing the right setup\n\nThe table below covers all common setups. Rows marked âœ… are implemented in this notebook; rows marked ðŸ“– are for reference only.\n\n| | Success metric | Target variable | LightGBM objective | Eval metric |\n|---|---|---|---|---|\n| âœ… | CTR | `ctr` (float) | `regression` | RMSE / MAE |\n| âœ… | Conversion | `has_conversion` (0/1) | `binary` | AUC / PR-AUC |\n| âœ… | Keyword ranking | any relevance label | `lambdarank` | NDCG@k |\n| ðŸ“– | ROAS / Profit | continuous value | `regression` or `tweedie` | RMSE |\n| ðŸ“– | Click volume | `clicks` (count) | `poisson` | â€” |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}