{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22c6ae9",
   "metadata": {},
   "source": [
    "# LightGBM Ads Tutorial\n",
    "\n",
    "End-to-end tutorial using synthetic keyword-ads performance data.\n",
    "\n",
    "**Models covered:**\n",
    "1. CTR prediction (regression)\n",
    "2. Conversion prediction (binary classification)\n",
    "3. Keyword ranking function (score-based)\n",
    "4. Learning-to-Rank with LambdaMART (group-split version)\n",
    "5. Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df58ec9",
   "metadata": {},
   "source": [
    "## 0) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27814e01",
   "metadata": {},
   "source": [
    "## 1) Create synthetic ads dataset\n",
    "\n",
    "Each row represents one `(given_word, keyword)` pair with features:\n",
    "- `similarity` – cosine-like similarity between the two words\n",
    "- `competition`, `impressions`, `clicks`, `cpc`, `cost`, `device`, `hour`\n",
    "\n",
    "Targets:\n",
    "- `ctr` – click-through rate (regression)\n",
    "- `has_conversion` – did it convert at least once? (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Load word lists from files\n",
    "with open(\"given_words.json\") as f:\n",
    "    given_words = np.array(json.load(f))\n",
    "\n",
    "with open(\"keywords.json\") as f:\n",
    "    keywords = np.array(json.load(f))\n",
    "\n",
    "print(f\"given_words : {len(given_words)}\")\n",
    "print(f\"keywords    : {len(keywords)}\")\n",
    "\n",
    "# ── Precompute real similarities ───────────────────────────────────────────────\n",
    "print(\"Encoding embeddings...\")\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "all_words = np.unique(np.concatenate([given_words, keywords]))\n",
    "vecs      = embed_model.encode(all_words.tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "vec_index = dict(zip(all_words, vecs))\n",
    "\n",
    "def real_similarity(given: np.ndarray, kw: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Vectorised cosine similarity for arrays of given/keyword strings.\"\"\"\n",
    "    g_vecs = np.stack([vec_index[g] for g in given])\n",
    "    k_vecs = np.stack([vec_index[k] for k in kw])\n",
    "    # row-wise dot product (vectors are already normalised)\n",
    "    return np.clip((g_vecs * k_vecs).sum(axis=1), 0.0, 1.0)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "def make_ads_dataset(n=100_000):\n",
    "    given = rng.choice(given_words, size=n)\n",
    "    kw    = rng.choice(keywords, size=n)\n",
    "\n",
    "    similarity  = real_similarity(given, kw)\n",
    "\n",
    "    impressions = rng.integers(50, 20000, size=n)\n",
    "    device      = rng.choice([\"mobile\", \"desktop\"], size=n, p=[0.7, 0.3])\n",
    "    hour        = rng.integers(0, 24, size=n)\n",
    "    competition = rng.uniform(0.1, 1.0, size=n)\n",
    "\n",
    "    cpc = np.clip(\n",
    "        0.2 + 2.0 * competition + 0.5 * (1 - similarity) + rng.normal(0, 0.15, size=n),\n",
    "        0.05, None\n",
    "    )\n",
    "\n",
    "    device_boost = np.where(device == \"mobile\", 0.02, 0.0)\n",
    "    hour_boost   = np.where((hour >= 19) & (hour <= 23), 0.01, 0.0)\n",
    "    ctr = np.clip(\n",
    "        0.01 + 0.10 * similarity + device_boost + hour_boost + rng.normal(0, 0.01, size=n),\n",
    "        0.0005, 0.30\n",
    "    )\n",
    "\n",
    "    clicks = rng.binomial(impressions, p=ctr)\n",
    "    cost   = clicks * cpc\n",
    "\n",
    "    conv_p = 1 / (1 + np.exp(-(-2.0 + 4.0 * similarity - 0.4 * cpc)))\n",
    "    conversions    = rng.binomial(np.maximum(clicks, 1), p=np.clip(conv_p, 0.0001, 0.8))\n",
    "    has_conversion = (conversions > 0).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"given_word\":     given,\n",
    "        \"keyword\":        kw,\n",
    "        \"similarity\":     similarity,\n",
    "        \"competition\":    competition,\n",
    "        \"impressions\":    impressions,\n",
    "        \"clicks\":         clicks,\n",
    "        \"cpc\":            cpc,\n",
    "        \"cost\":           cost,\n",
    "        \"device\":         device,\n",
    "        \"hour\":           hour,\n",
    "        \"ctr\":            np.where(impressions > 0, clicks / impressions, 0.0),\n",
    "        \"has_conversion\": has_conversion,\n",
    "        \"conversions\":    conversions,\n",
    "    })\n",
    "\n",
    "df = make_ads_dataset(100_000)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21903c20",
   "metadata": {},
   "source": [
    "## 2) Prepare features\n",
    "\n",
    "LightGBM handles categorical features natively when they are `pandas.Categorical` dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"given_word\", \"keyword\", \"similarity\", \"competition\",\n",
    "    \"impressions\", \"clicks\", \"cpc\", \"cost\", \"device\", \"hour\"\n",
    "]\n",
    "CAT_COLS = [\"given_word\", \"keyword\", \"device\"]\n",
    "\n",
    "X = df[FEATURE_COLS].copy()\n",
    "for c in CAT_COLS:\n",
    "    X[c] = X[c].astype(\"category\")\n",
    "\n",
    "y_ctr  = df[\"ctr\"].values\n",
    "y_conv = df[\"has_conversion\"].values\n",
    "\n",
    "X_train, X_test, y_ctr_train, y_ctr_test = train_test_split(\n",
    "    X, y_ctr, test_size=0.2, random_state=42\n",
    ")\n",
    "# reuse the same split indices for the conversion target\n",
    "y_conv_train = y_conv[X_train.index]\n",
    "y_conv_test  = y_conv[X_test.index]\n",
    "\n",
    "print(f\"Train: {X_train.shape}  |  Test: {X_test.shape}\")\n",
    "print(f\"Conversion rate (train): {y_conv_train.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde0599",
   "metadata": {},
   "source": [
    "## 3) Model A — CTR prediction (regression)\n",
    "\n",
    "CTR is continuous and bounded in (0, 1). We weight each sample by `impressions` so high-volume rows have more influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ef3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = lgb.LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "reg.fit(\n",
    "    X_train, y_ctr_train,\n",
    "    sample_weight=X_train[\"impressions\"],\n",
    "    eval_set=[(X_test, y_ctr_test)],\n",
    "    eval_sample_weight=[X_test[\"impressions\"]],\n",
    "    eval_metric=\"l2\",\n",
    "    categorical_feature=CAT_COLS,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=80, verbose=False),\n",
    "               lgb.log_evaluation(period=200)],\n",
    ")\n",
    "\n",
    "pred_ctr = reg.predict(X_test)\n",
    "rmse = mean_squared_error(y_ctr_test, pred_ctr) ** 0.5\n",
    "print(f\"\\nCTR RMSE : {rmse:.6f}\")\n",
    "print(f\"Best iter: {reg.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cfe2d",
   "metadata": {},
   "source": [
    "## 4) Model B — Conversion prediction (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe96b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train, y_conv_train,\n",
    "    eval_set=[(X_test, y_conv_test)],\n",
    "    eval_metric=\"auc\",\n",
    "    categorical_feature=CAT_COLS,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "               lgb.log_evaluation(period=200)],\n",
    ")\n",
    "\n",
    "proba = clf.predict_proba(X_test)[:, 1]\n",
    "print(f\"\\nAUC   : {roc_auc_score(y_conv_test, proba):.4f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_conv_test, proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4502d",
   "metadata": {},
   "source": [
    "## 5) Rank keywords for a given word\n",
    "\n",
    "For a new `given_word`, score a list of candidate keywords using:\n",
    "- `pred_ctr` from the regression model\n",
    "- `pred_conv_prob` from the classifier\n",
    "- `score = pred_ctr × pred_conv_prob` (customize to ROAS, profit, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926eba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_keywords_for_given(given_word: str, candidates: dict, base_features: dict) -> pd.DataFrame:\n",
    "    \"\"\"Score and rank candidate keywords for a given word.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    given_word    : The query / seed word.\n",
    "    candidates    : Dict of {keyword: similarity_score} pairs.\n",
    "    base_features : Dict of feature values shared across all candidates\n",
    "                    (all cols except given_word, keyword, similarity).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame sorted by score descending.\n",
    "    \"\"\"\n",
    "    rows = [\n",
    "        {**base_features, \"given_word\": given_word, \"keyword\": kw, \"similarity\": sim}\n",
    "        for kw, sim in candidates.items()\n",
    "    ]\n",
    "    Xcand = pd.DataFrame(rows)[FEATURE_COLS]\n",
    "    for c in CAT_COLS:\n",
    "        Xcand[c] = Xcand[c].astype(\"category\")\n",
    "\n",
    "    ctr_hat  = reg.predict(Xcand)\n",
    "    conv_hat = clf.predict_proba(Xcand)[:, 1]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"given_word\":     given_word,\n",
    "        \"keyword\":        list(candidates.keys()),\n",
    "        \"similarity\":     list(candidates.values()),\n",
    "        \"pred_ctr\":       ctr_hat,\n",
    "        \"pred_conv_prob\": conv_hat,\n",
    "        \"score\":          ctr_hat * conv_hat,\n",
    "    }).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Per-keyword similarity scores (in practice, compute these from word embeddings)\n",
    "candidates = {\n",
    "    \"white sneakers\":    0.95,\n",
    "    \"running shoes\":     0.82,\n",
    "    \"canvas shoes\":      0.74,\n",
    "    \"hiking boots\":      0.55,\n",
    "    \"yoga mat\":          0.20,\n",
    "    \"leather wallet\":    0.10,\n",
    "    \"wireless earbuds\":  0.08,\n",
    "    \"gaming mouse\":      0.05,\n",
    "}\n",
    "\n",
    "base = {\n",
    "    \"competition\": 0.6,\n",
    "    \"impressions\": 5000,\n",
    "    \"clicks\":      0,\n",
    "    \"cpc\":         2.0,\n",
    "    \"cost\":        0.0,\n",
    "    \"device\":      \"mobile\",\n",
    "    \"hour\":        21,\n",
    "}\n",
    "\n",
    "ranked = rank_keywords_for_given(\"sneakers\", candidates, base)\n",
    "ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32e47d",
   "metadata": {},
   "source": [
    "## 6) Learning-to-Rank with LambdaMART\n",
    "\n",
    "A proper LambdaMART setup requires:\n",
    "1. **Group-based train/test split** — keep all rows for a `given_word` in the same split.\n",
    "2. **Group sizes array** — number of candidate keywords per query, in order.\n",
    "3. **Relevance labels** — here we use `ctr`; in production use ROAS or conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "\n",
    "# ── 6a) Group-based train/test split ──────────────────────────────────────────\n",
    "unique_given = df[\"given_word\"].unique()\n",
    "rng_split    = np.random.default_rng(0)\n",
    "rng_split.shuffle(unique_given)\n",
    "\n",
    "split_idx    = int(len(unique_given) * 0.8)\n",
    "train_words  = set(unique_given[:split_idx])\n",
    "test_words   = set(unique_given[split_idx:])\n",
    "\n",
    "df_rank = df.sort_values(\"given_word\").copy()\n",
    "\n",
    "mask_train = df_rank[\"given_word\"].isin(train_words)\n",
    "df_r_train = df_rank[mask_train].copy()\n",
    "df_r_test  = df_rank[~mask_train].copy()\n",
    "\n",
    "print(f\"Ranker train rows: {len(df_r_train)}  |  test rows: {len(df_r_test)}\")\n",
    "print(f\"Train given_words: {sorted(train_words)}\")\n",
    "print(f\"Test  given_words: {sorted(test_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f3211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 6b) Build feature matrices and group size arrays ──────────────────────────\n",
    "def bin_ctr(ctr_values: np.ndarray, n_bins: int = 5) -> np.ndarray:\n",
    "    \"\"\"Convert continuous CTR into integer relevance grades (0 to n_bins-1).\"\"\"\n",
    "    bins = np.quantile(ctr_values, np.linspace(0, 1, n_bins + 1))\n",
    "    bins = np.unique(bins)  # remove duplicates if any\n",
    "    return np.digitize(ctr_values, bins[1:-1]).astype(int)\n",
    "\n",
    "def build_rank_arrays(subset: pd.DataFrame):\n",
    "    Xr = subset[FEATURE_COLS].copy()\n",
    "    for c in CAT_COLS:\n",
    "        Xr[c] = Xr[c].astype(\"category\")\n",
    "    y      = bin_ctr(subset[\"ctr\"].values)   # integer grades required by LambdaMART\n",
    "    groups = subset.groupby(\"given_word\", sort=True).size().tolist()\n",
    "    return Xr, y, groups\n",
    "\n",
    "Xr_train, yr_train, groups_train = build_rank_arrays(df_r_train)\n",
    "Xr_test,  yr_test,  groups_test  = build_rank_arrays(df_r_test)\n",
    "\n",
    "print(f\"Label range: {yr_train.min()} – {yr_train.max()}  (grades 0–4)\")\n",
    "print(f\"Group sizes (train): {groups_train}\")\n",
    "print(f\"Group sizes (test) : {groups_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 6c) Train LambdaMART ranker ───────────────────────────────────────────────\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "ranker.fit(\n",
    "    Xr_train, yr_train,\n",
    "    group=groups_train,\n",
    "    eval_set=[(Xr_test, yr_test)],\n",
    "    eval_group=[groups_test],\n",
    "    eval_at=[3, 5, 10],\n",
    "    categorical_feature=CAT_COLS,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=80, verbose=False),\n",
    "               lgb.log_evaluation(period=200)],\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {ranker.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf40b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 6d) Inspect ranker scores for one test given_word ─────────────────────────\n",
    "sample_word = list(test_words)[0]\n",
    "df_sample   = df_r_test[df_r_test[\"given_word\"] == sample_word].copy()\n",
    "\n",
    "Xs = df_sample[FEATURE_COLS].copy()\n",
    "for c in CAT_COLS:\n",
    "    Xs[c] = Xs[c].astype(\"category\")\n",
    "\n",
    "df_sample[\"ranker_score\"] = ranker.predict(Xs)\n",
    "df_sample[[\"given_word\", \"keyword\", \"ctr\", \"ranker_score\"]] \\\n",
    "    .sort_values(\"ranker_score\", ascending=False) \\\n",
    "    .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7c66d",
   "metadata": {},
   "source": [
    "## 7) Feature importance\n",
    "\n",
    "Using **gain** (total reduction in loss attributed to each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025469cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_importance(model, title: str):\n",
    "    fi = pd.DataFrame({\n",
    "        \"feature\":    model.feature_name_,\n",
    "        \"importance\": model.booster_.feature_importance(importance_type=\"gain\"),\n",
    "    }).sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(fi.to_string(index=False))\n",
    "    return fi\n",
    "\n",
    "fi_reg    = show_importance(reg,    \"CTR Regression\")\n",
    "fi_clf    = show_importance(clf,    \"Conversion Classifier\")\n",
    "fi_ranker = show_importance(ranker, \"LambdaMART Ranker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270d690",
   "metadata": {},
   "source": [
    "## Quick-reference: choosing the right setup\n",
    "\n",
    "| Success metric | Target variable | LightGBM objective | Eval metric |\n",
    "|---|---|---|---|\n",
    "| CTR | `ctr` (float) | `regression` | RMSE / MAE |\n",
    "| Conversion | `has_conversion` (0/1) | `binary` | AUC / PR-AUC |\n",
    "| ROAS / Profit | continuous value | `regression` or `tweedie` | RMSE |\n",
    "| Click volume | `clicks` (count) | `poisson` | — |\n",
    "| Keyword ranking | any relevance label | `lambdarank` | NDCG@k |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
